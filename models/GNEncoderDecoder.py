from base.base_model import BaseModel
import tensorflow as tf

from graph_nets import modules
from graph_nets import utils_tf
import sonnet as snt

NUM_LAYERS = 2
LATENT_SIZE = 16

class EncoderDecoder(BaseModel):
    def __init__(self, config, edge_output_size, node_output_size, global_output_size, name="EncoderDecoder"):
        super(EncoderDecoder, self).__init__(config)

        self.build_model(edge_output_size, node_output_size, global_output_size, name="EncoderDecoder")
        self.init_saver()


    def build_model(self, edge_output_size=None, node_output_size=None, global_output_size=None, name=None):
        self.is_training = tf.placeholder(tf.bool)

        self._encoder = MLPGraphIndependent()
        self._core = MLPGraphNetwork()
        self._decoder = MLPGraphIndependent()
        # Transforms the outputs into the appropriate shapes.
        if edge_output_size is None:
            edge_fn = None
        else:
            edge_fn = lambda: snt.Linear(edge_output_size, name="edge_output")
        if node_output_size is None:
            node_fn = None
        else:
            node_fn = lambda: snt.Linear(node_output_size, name="node_output")
        if global_output_size is None:
            global_fn = None
        else:
            global_fn = lambda: snt.Linear(global_output_size, name="global_output")

        #with self._enter_variable_scope():
        self._output_transform = modules.GraphIndependent(edge_fn, node_fn, global_fn)


        # todo
        with tf.name_scope("loss"):
            self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=d2))
            self.train_step = tf.train.AdamOptimizer(self.config.learning_rate).minimize(self.cross_entropy,
                                                                                         global_step=self.global_step_tensor)
            correct_prediction = tf.equal(tf.argmax(d2, 1), tf.argmax(self.y, 1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


    def init_saver(self):
        # here you initialize the tensorflow saver that will be used in saving the checkpoints.
        self.saver = tf.train.Saver(max_to_keep=self.config.max_to_keep)


    def make_mlp_model(self):
      """Instantiates a new MLP, followed by LayerNorm.

      The parameters of each new MLP are not shared with others generated by
      this function.

      Returns:
        A Sonnet module which contains the MLP and LayerNorm.
      """
      return snt.Sequential([
          snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),
          snt.LayerNorm()
      ])


class MLPGraphIndependent(snt.AbstractModule):
    """GraphIndependent with MLP edge, node, and global models."""

    def __init__(self, name="MLPGraphIndependent"):
        super(MLPGraphIndependent, self).__init__(name=name)
        with self._enter_variable_scope():
            self._network = modules.GraphIndependent(edge_model_fn=make_mlp_model, node_model_fn=make_mlp_model,
                global_model_fn=make_mlp_model)

    def _build(self, inputs):
        return self._network(inputs)

class MLPGraphNetwork(snt.AbstractModule):
    """GraphNetwork with MLP edge, node, and global models."""

    def __init__(self, name="MLPGraphNetwork"):
        super(MLPGraphNetwork, self).__init__(name=name)
        with self._enter_variable_scope():
            self._network = modules.GraphNetwork(make_mlp_model, make_mlp_model, make_mlp_model)

    def _build(self, inputs):
        return self._network(inputs)

